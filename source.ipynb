{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "10495750",
      "metadata": {},
      "source": [
        "# Code Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5444dc46",
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "import subprocess\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "15885887",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, World!\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello, World!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d15eabaa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq API Key exists and begins gsk_\n"
          ]
        }
      ],
      "source": [
        "load_dotenv(override=True)\n",
        "\n",
        "groq_api_key = os.getenv('GROQ_API_KEY')\n",
        "\n",
        "\n",
        "if groq_api_key:\n",
        "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
        "else:\n",
        "    print(\"Groq API Key not set (and this is optional)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "cf469205",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to client libraries\n",
        "\n",
        "groq_url = \"https://api.groq.com/openai/v1\"\n",
        "ollama_url = \"http://localhost:11434/v1\"\n",
        "\n",
        "\n",
        "groq = OpenAI(api_key=groq_api_key, base_url=groq_url) \n",
        "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "f4bc502b",
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [\"qwen2.5:3b\", \"llama3.2:1b\", \"gemma3:270m\", \"openai/gpt-oss-120b\", \"qwen/qwen3-32b\", \"meta-llama/llama-4-maverick-17b\", \"moonshotai/kimi-k2-instruct-0905\", \"openai/gpt-oss-20b\"]\n",
        "\n",
        "clients = { \"openai/gpt-oss-120b\": groq, \"qwen/qwen3-32b\": groq, \"meta-llama/llama-4-maverick-17b-128e-instruct\": groq, \"moonshotai/kimi-k2-instruct-0905\": groq, \"openai/gpt-oss-20b\": groq, \"qwen2.5:3b\": ollama, \"llama3.2:1b\": ollama, \"gemma3:270m\": ollama}\n",
        "\n",
        "# Want to keep costs ultra-low? Replace this with models of your choice, using the examples from yesterday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f6c912a7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'os': {'system': 'Windows',\n",
              "  'arch': 'AMD64',\n",
              "  'release': '11',\n",
              "  'version': '10.0.26200',\n",
              "  'kernel': '11',\n",
              "  'distro': None,\n",
              "  'wsl': False,\n",
              "  'rosetta2_translated': False,\n",
              "  'target_triple': 'x86_64-w64-mingw32'},\n",
              " 'package_managers': ['winget'],\n",
              " 'cpu': {'brand': '', 'cores_logical': 12, 'cores_physical': 10, 'simd': []},\n",
              " 'toolchain': {'compilers': {'gcc': 'gcc.EXE (Rev2, Built by MSYS2 project) 14.2.0',\n",
              "   'g++': 'g++.EXE (Rev2, Built by MSYS2 project) 14.2.0',\n",
              "   'clang': '',\n",
              "   'msvc_cl': ''},\n",
              "  'build_tools': {'cmake': '', 'ninja': '', 'make': ''},\n",
              "  'linkers': {'ld_lld': ''}}}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from system_info import retrieve_system_info\n",
        "\n",
        "system_info = retrieve_system_info()\n",
        "system_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a5ea427b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use g++ (available on your system); system_info shows clang is not installed\n",
        "compile_command = [\n",
        "    \"g++\",\n",
        "    \"-std=c++17\",\n",
        "    \"-Ofast\",\n",
        "    \"-march=native\",\n",
        "    \"-flto\",\n",
        "    \"-fvisibility=hidden\",\n",
        "    \"-DNDEBUG\",\n",
        "    \"main.cpp\",\n",
        "    \"-o\",\n",
        "    \"main.exe\"\n",
        "]\n",
        "\n",
        "run_command = [\"./main.exe\"] if os.name != \"nt\" else [\"main.exe\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "a1e49184",
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "Your task is to convert Python code into high performance C++ code.\n",
        "Respond only with C++ code. Do not provide any explanation other than occasional comments.\n",
        "The C++ response needs to produce an identical output in the fastest possible time.\n",
        "\"\"\"\n",
        "\n",
        "def user_prompt_for(python):\n",
        "    return f\"\"\"\n",
        "Port this Python code to C++ with the fastest possible implementation that produces identical output in the least time.\n",
        "The system information is:\n",
        "{system_info}\n",
        "Your response will be written to a file called main.cpp and then compiled and executed; the compilation command is:\n",
        "{compile_command}\n",
        "Respond only with C++ code.\n",
        "Python code to port:\n",
        "\n",
        "```python\n",
        "{python}\n",
        "```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a716a478",
      "metadata": {},
      "outputs": [],
      "source": [
        "def messages_for(python):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
        "    ]\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e8c2d306",
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_output(cpp):\n",
        "    with open(\"main.cpp\", \"w\") as f:\n",
        "        f.write(cpp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "69e035b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def port(model, python):\n",
        "    client = clients[model]\n",
        "    reasoning_effort = \"high\" if 'gpt' in model else None\n",
        "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
        "    reply = response.choices[0].message.content\n",
        "    reply = reply.replace('```cpp','').replace('```','')\n",
        "    write_output(reply)\n",
        "    return reply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "ccb07262",
      "metadata": {},
      "outputs": [],
      "source": [
        "pi = \"\"\"\n",
        "import time\n",
        "\n",
        "def calculate(iterations, param1, param2):\n",
        "    result = 1.0\n",
        "    for i in range(1, iterations+1):\n",
        "        j = i * param1 - param2\n",
        "        result -= (1/j)\n",
        "        j = i * param1 + param2\n",
        "        result += (1/j)\n",
        "    return result\n",
        "\n",
        "start_time = time.time()\n",
        "result = calculate(200_000_000, 4, 1) * 4\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Result: {result:.12f}\")\n",
        "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "dd32ab2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_python(code):\n",
        "    globals_dict = {\"__builtins__\": __builtins__}\n",
        "\n",
        "    buffer = io.StringIO()\n",
        "    old_stdout = sys.stdout\n",
        "    sys.stdout = buffer\n",
        "\n",
        "    try:\n",
        "        exec(code, globals_dict)\n",
        "        output = buffer.getvalue()\n",
        "    except Exception as e:\n",
        "        output = f\"Error: {e}\"\n",
        "    finally:\n",
        "        sys.stdout = old_stdout\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "d1987c64",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update compile_and_run to return string output instead of printing\n",
        "def compile_and_run_and_capture():\n",
        "    output_log = \"\"\n",
        "    try:\n",
        "        # 1. Compile\n",
        "        # Capture stderr in case of compilation errors\n",
        "        comp_result = subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
        "        output_log += \"‚úÖ Compilation Successful.\\n\"\n",
        "        \n",
        "        # 2. Run (3 times for consistency, as per your original logic)\n",
        "        output_log += \"üöÄ Running benchmark (3 iterations)...\\n\"\n",
        "        output_log += \"-\" * 30 + \"\\n\"\n",
        "        \n",
        "        for i in range(3):\n",
        "            # Run the executable\n",
        "            run_result = subprocess.run(run_command, check=True, text=True, capture_output=True)\n",
        "            output_log += f\"Run #{i+1}:\\n{run_result.stdout}\\n\"\n",
        "            \n",
        "        return output_log\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle compilation or execution errors\n",
        "        error_msg = f\"‚ùå Error occurred:\\n{e.stderr}\"\n",
        "        if hasattr(e, 'stdout') and e.stdout:\n",
        "             error_msg += f\"\\nOutput before crash:\\n{e.stdout}\"\n",
        "        return error_msg\n",
        "\n",
        "# Wrapper to save manual edits from the UI before running\n",
        "def save_compile_run_wrapper(cpp_code):\n",
        "    write_output(cpp_code) # Uses your existing write_output function\n",
        "    return compile_and_run_and_capture()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "2b1a4190",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'‚úÖ Compilation Successful.\\nüöÄ Running benchmark (3 iterations)...\\n------------------------------\\nRun #1:\\nResult: 3.141592656542\\nExecution Time: 0.263195 seconds\\n\\nRun #2:\\nResult: 3.141592656542\\nExecution Time: 0.448771 seconds\\n\\nRun #3:\\nResult: 3.141592656542\\nExecution Time: 0.858004 seconds\\n\\n'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compile_and_run_and_capture()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "203949c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7862\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
            "    yield\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
            "    resp = self._pool.handle_request(req)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
            "    raise exc from None\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
            "    response = connection.handle_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 101, in handle_request\n",
            "    raise exc\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 78, in handle_request\n",
            "    stream = self._connect(request)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 124, in _connect\n",
            "    stream = self._network_backend.connect_tcp(**kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py\", line 207, in connect_tcp\n",
            "    with map_exceptions(exc_map):\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sajid Manzoor Bhatti\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(value)\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
            "    raise to_exc(exc) from exc\n",
            "httpcore.ConnectError: [Errno 11001] getaddrinfo failed\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 982, in request\n",
            "    response = self._client.send(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
            "    response = self._send_handling_auth(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
            "    response = self._send_handling_redirects(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
            "    response = self._send_single_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
            "    response = transport.handle_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 249, in handle_request\n",
            "    with map_httpcore_exceptions():\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sajid Manzoor Bhatti\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(value)\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
            "    raise mapped_exc(message) from exc\n",
            "httpx.ConnectError: [Errno 11001] getaddrinfo failed\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sajid Manzoor Bhatti\\AppData\\Local\\Temp\\ipykernel_15416\\2505858948.py\", line 4, in port\n",
            "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1156, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\LEARNING\\LLM_Engineering\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1014, in request\n",
            "    raise APIConnectionError(request=request) from err\n",
            "openai.APIConnectionError: Connection error.\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Using a theme for a modern look\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Python to C++ Converter\") as ui:\n",
        "    \n",
        "    gr.Markdown(\"\"\"\n",
        "    # ‚ö° Python to C++ Transpiler & Benchmark\n",
        "    Convert Python algorithms to high-performance C++ using LLMs and compare execution speeds directly.\n",
        "    \"\"\")\n",
        "\n",
        "    # --- Settings Row ---\n",
        "    with gr.Row(variant=\"panel\"):\n",
        "        model_selector = gr.Dropdown(\n",
        "            choices=models, \n",
        "            label=\"Select LLM Model\", \n",
        "            value=models[0],\n",
        "            info=\"Choose the model to perform the translation.\"\n",
        "        )\n",
        "\n",
        "    # --- Main Interface Columns ---\n",
        "    with gr.Row():\n",
        "        # LEFT COLUMN: PYTHON (Input & Baseline)\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### üêç Python Source\")\n",
        "            python_input = gr.Code(\n",
        "                language=\"python\", \n",
        "                label=\"Input Code\", \n",
        "                value=pi, \n",
        "                lines=20\n",
        "            )\n",
        "            \n",
        "            with gr.Row():\n",
        "                run_py_btn = gr.Button(\"‚ñ∂ Run Python (Baseline)\")\n",
        "            \n",
        "            python_output = gr.Textbox(\n",
        "                label=\"Python Execution Output\", \n",
        "                lines=8, \n",
        "                placeholder=\"Python output will appear here...\"\n",
        "            )\n",
        "\n",
        "        # RIGHT COLUMN: C++ (Output & High Performance)\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### ‚öôÔ∏è C++ Generated\")\n",
        "            # Using gr.Code allows you to edit the C++ before running if needed\n",
        "            cpp_output = gr.Code(\n",
        "                language=\"cpp\", \n",
        "                label=\"Generated C++ Code\", \n",
        "                lines=20, \n",
        "                interactive=True \n",
        "            )\n",
        "            \n",
        "            with gr.Row():\n",
        "                convert_btn = gr.Button(\"‚ú® Convert to C++\", variant=\"primary\")\n",
        "                run_cpp_btn = gr.Button(\"‚ö° Compile & Run C++\", variant=\"secondary\")\n",
        "            \n",
        "            cpp_result_output = gr.Textbox(\n",
        "                label=\"C++ Compilation & Execution Output\", \n",
        "                lines=8,\n",
        "                placeholder=\"Benchmark results will appear here...\"\n",
        "            )\n",
        "\n",
        "    # --- Event Wiring ---\n",
        "    \n",
        "    # 1. Run Python Code\n",
        "    run_py_btn.click(\n",
        "        fn=run_python, \n",
        "        inputs=[python_input], \n",
        "        outputs=[python_output]\n",
        "    )\n",
        "    \n",
        "    # 2. Convert Python to C++ (calls your existing 'port' function)\n",
        "    convert_btn.click(\n",
        "        fn=port, \n",
        "        inputs=[model_selector, python_input], \n",
        "        outputs=[cpp_output]\n",
        "    )\n",
        "    \n",
        "    # 3. Compile & Run C++ (Saves code from box -> Compiles -> Runs)\n",
        "    run_cpp_btn.click(\n",
        "        fn=save_compile_run_wrapper, \n",
        "        inputs=[cpp_output], \n",
        "        outputs=[cpp_result_output]\n",
        "    )\n",
        "\n",
        "ui.launch(inbrowser=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3beaf881",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
